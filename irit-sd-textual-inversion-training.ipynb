{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":["D633UIuGgs6M"],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"00edc26e07c54b0f9b49c0c7e91597d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2784cd443fee4294887f8444ca28f4d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45d9d9d9c6404dd0a7fb90f4cc6f3c5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"660aec172b104d45a7fe6f79d8ee4313":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f2cf3db2f3a4409a5fa4337c629af35","placeholder":"​","style":"IPY_MODEL_00edc26e07c54b0f9b49c0c7e91597d5","value":" 12/12 [00:00&lt;00:00, 354.30it/s]"}},"674afdddd45346bda4aec021c1235d8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7236089783a14bceb1caf055a7454ed2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_674afdddd45346bda4aec021c1235d8a","placeholder":"​","style":"IPY_MODEL_45d9d9d9c6404dd0a7fb90f4cc6f3c5a","value":"Fetching 12 files: 100%"}},"984c4d2cc10c433e976cb90c658f486f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f2cf3db2f3a4409a5fa4337c629af35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7005e8a2ae4974b805e1cc06480992":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8713261d79d4f5aad70e628ce181b5a","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_984c4d2cc10c433e976cb90c658f486f","value":12}},"e8713261d79d4f5aad70e628ce181b5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9d96d0100c94525bbf4e764219d3ec2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7236089783a14bceb1caf055a7454ed2","IPY_MODEL_bd7005e8a2ae4974b805e1cc06480992","IPY_MODEL_660aec172b104d45a7fe6f79d8ee4313"],"layout":"IPY_MODEL_2784cd443fee4294887f8444ca28f4d6"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11702528,"sourceType":"datasetVersion","datasetId":7331215}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Initial setup","metadata":{"id":"KbzZ9xe6dWwf"}},{"cell_type":"code","source":"#@title Install the required libs\n!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n!pip install -qq accelerate transformers ftfy\n!pip install --upgrade peft","metadata":{"id":"30lu8LWXmg5j","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:08:16.782750Z","iopub.execute_input":"2025-05-05T06:08:16.783180Z","iopub.status.idle":"2025-05-05T06:09:54.212830Z","shell.execute_reply.started":"2025-05-05T06:08:16.783131Z","shell.execute_reply":"2025-05-05T06:09:54.211835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title [Optional] Install xformers for faster and memory efficient training\n#@markdown Acknowledgement: The xformers wheel are taken from [TheLastBen/fast-stable-diffusion](https://github.com/TheLastBen/fast-stable-diffusion). Thanks a lot for building these wheels!\n\n!pip install -U --pre triton\n\nfrom subprocess import getoutput\nfrom IPython.display import HTML\nfrom IPython.display import clear_output\nimport time\n\ns = getoutput('nvidia-smi')\nif 'T4' in s:\n  gpu = 'T4'\nelif 'P100' in s:\n  gpu = 'P100'\nelif 'V100' in s:\n  gpu = 'V100'\nelif 'A100' in s:\n  gpu = 'A100'\n\nwhile True:\n    try:\n        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n        break\n    except:\n        pass\n    print('[1;31mit seems that your GPU is not supported at the moment')\n    time.sleep(5)\n\nif (gpu=='T4'):\n  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n\nelif (gpu=='P100'):\n  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n\nelif (gpu=='V100'):\n  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n\nelif (gpu=='A100'):\n  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl","metadata":{"id":"Ji4BmlYPDqD4","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:09:54.214501Z","iopub.execute_input":"2025-05-05T06:09:54.214760Z","iopub.status.idle":"2025-05-05T06:10:10.933213Z","shell.execute_reply.started":"2025-05-05T06:09:54.214737Z","shell.execute_reply":"2025-05-05T06:10:10.932472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Import required libraries\nimport argparse\nimport itertools\nimport math\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nimport PIL\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n\n# merge multiple images into a grid layout\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid","metadata":{"id":"1_h0kO-VnQog","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:10.934230Z","iopub.execute_input":"2025-05-05T06:10:10.934905Z","iopub.status.idle":"2025-05-05T06:10:37.734392Z","shell.execute_reply.started":"2025-05-05T06:10:10.934880Z","shell.execute_reply":"2025-05-05T06:10:37.733599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Settings for teaching your new concept","metadata":{"id":"Yl3r7A_3ASxm"}},{"cell_type":"markdown","source":"### Get the training images:","metadata":{"id":"BU5hQlD4ovdQ"}},{"cell_type":"code","source":"#@markdown `images_path` is a path to directory containing the training images. It could\nimages_path = \"/kaggle/input/irit-images\" #@param {type:\"string\"}\nwhile not os.path.exists(str(images_path)):\n  print('The images_path specified does not exist, use the colab file explorer to copy the path :')\n  images_path=input(\"\")\nsave_path = images_path","metadata":{"id":"R1NpK1tSiH8a","outputId":"39716245-772e-4f7d-db5a-4945adf2db50","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:37.736395Z","iopub.execute_input":"2025-05-05T06:10:37.736936Z","iopub.status.idle":"2025-05-05T06:10:37.741609Z","shell.execute_reply.started":"2025-05-05T06:10:37.736916Z","shell.execute_reply":"2025-05-05T06:10:37.740843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"####  Setup and check the images you have just added","metadata":{"id":"_AAOoKZNpdSN"}},{"cell_type":"code","source":"images = []\nfor file_path in os.listdir(save_path):\n  try:\n      image_path = os.path.join(save_path, file_path)\n      images.append(Image.open(image_path).resize((512, 512)))\n  except:\n    print(f\"{image_path} is not a valid image, please make sure to remove this file from the directory otherwise the training could fail.\")\nimage_grid(images, 1, len(images))","metadata":{"id":"60jVYSk0BGC8","outputId":"ebe2b650-fea2-46a4-8a7e-b86dd3029b29","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:37.742424Z","iopub.execute_input":"2025-05-05T06:10:37.742684Z","iopub.status.idle":"2025-05-05T06:10:40.108654Z","shell.execute_reply.started":"2025-05-05T06:10:37.742661Z","shell.execute_reply":"2025-05-05T06:10:40.107842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Settings for your newly created concept\n#@markdown `what_to_teach`: what is it that you are teaching? `object` enables you to teach the model a new object to be used, `style` allows you to teach the model a new style one can use.\nwhat_to_teach = \"object\" #@param [\"object\", \"style\"]\n#@markdown `placeholder_token` is the token you are going to use to represent your new concept (so when you prompt the model, you will say \"A `<my-placeholder-token>` in an amusement park\"). We use angle brackets to differentiate a token from other words/tokens, to avoid collision.\nplaceholder_token = \"sks\" #@param {type:\"string\"}\n#@markdown `initializer_token` is a word that can summarise what your new concept is, to be used as a starting point\ninitializer_token = \"woman\" #@param {type:\"string\"}","metadata":{"id":"8i_vLTBxAXpE","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:40.109778Z","iopub.execute_input":"2025-05-05T06:10:40.110122Z","iopub.status.idle":"2025-05-05T06:10:40.115788Z","shell.execute_reply.started":"2025-05-05T06:10:40.110091Z","shell.execute_reply":"2025-05-05T06:10:40.114901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Teach the model a new concept (fine-tuning with textual inversion)\nExecute this this sequence of cells to run the training process. The whole process may take from 1-4 hours. (Open this block if you are interested in how this process works under the hood or if you want to change advanced training settings or hyperparameters)","metadata":{"id":"D633UIuGgs6M"}},{"cell_type":"markdown","source":"### Create Dataset","metadata":{"id":"EuFP688UEwQR"}},{"cell_type":"code","source":"#@title Setup the prompt templates for training\nimagenet_templates_small = [\n    'a photo of {} woman',\n    'a portrait of {}',\n    'a closeup photo of {}',\n    'a picture of {} woman',\n    'a professional headshot of {}',\n]\n","metadata":{"id":"u4c1vbVfnmLf","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:40.116683Z","iopub.execute_input":"2025-05-05T06:10:40.117134Z","iopub.status.idle":"2025-05-05T06:10:40.133312Z","shell.execute_reply.started":"2025-05-05T06:10:40.117101Z","shell.execute_reply":"2025-05-05T06:10:40.132673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Setup the dataset\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"bilinear\": PIL.Image.BILINEAR,\n            \"bicubic\": PIL.Image.BICUBIC,\n            \"lanczos\": PIL.Image.LANCZOS,\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            h, w, = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example","metadata":{"id":"2ntpEpVfnd-0","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:40.134145Z","iopub.execute_input":"2025-05-05T06:10:40.134670Z","iopub.status.idle":"2025-05-05T06:10:40.147802Z","shell.execute_reply.started":"2025-05-05T06:10:40.134644Z","shell.execute_reply":"2025-05-05T06:10:40.147156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setting up the model","metadata":{"id":"TmrXxJ-Eijwb"}},{"cell_type":"code","source":"pretrained_model_name_or_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\n#@title Load the tokenizer and add the placeholder token as a additional special token.\ntokenizer = CLIPTokenizer.from_pretrained(\n    pretrained_model_name_or_path,\n    subfolder=\"tokenizer\",\n)\n\n# Add the placeholder token in tokenizer\nnum_added_tokens = tokenizer.add_tokens(placeholder_token)\nif num_added_tokens == 0:\n    raise ValueError(\n        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n        \" `placeholder_token` that is not already in the tokenizer.\"\n    )","metadata":{"id":"gIFaJum5nqeo","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:40.148648Z","iopub.execute_input":"2025-05-05T06:10:40.148883Z","iopub.status.idle":"2025-05-05T06:10:44.579177Z","shell.execute_reply.started":"2025-05-05T06:10:40.148864Z","shell.execute_reply":"2025-05-05T06:10:44.578498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Get token ids for our placeholder and initializer token. This code block will complain if initializer string is not a single token\n# Convert the initializer_token, placeholder_token to ids\ntoken_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n# Check if initializer_token is a single token or a sequence of tokens\nif len(token_ids) > 1:\n    raise ValueError(\"The initializer token must be a single token.\")\n\ninitializer_token_id = token_ids[0]\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)","metadata":{"id":"5jgTNr7roCnV","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:44.581847Z","iopub.execute_input":"2025-05-05T06:10:44.582057Z","iopub.status.idle":"2025-05-05T06:10:44.587279Z","shell.execute_reply.started":"2025-05-05T06:10:44.582042Z","shell.execute_reply":"2025-05-05T06:10:44.586644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Load the Stable Diffusion model\n# Load models and create wrapper for stable diffusion\n# pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)\n# del pipeline\ntext_encoder = CLIPTextModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n)\nvae = AutoencoderKL.from_pretrained(\n    \"stabilityai/sd-vae-ft-mse\"\n)\nunet = UNet2DConditionModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"unet\"\n)","metadata":{"id":"27Ip3q9YoFut","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:10:44.588458Z","iopub.execute_input":"2025-05-05T06:10:44.588732Z","iopub.status.idle":"2025-05-05T06:11:00.229918Z","shell.execute_reply.started":"2025-05-05T06:10:44.588700Z","shell.execute_reply":"2025-05-05T06:11:00.229350Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have added the `placeholder_token` in the `tokenizer` so we resize the token embeddings here, this will a new embedding vector in the token embeddings for our `placeholder_token`","metadata":{"id":"GD5MU6EzFe27"}},{"cell_type":"code","source":"text_encoder.resize_token_embeddings(len(tokenizer))","metadata":{"id":"24-9I6mIoORT","outputId":"2140b292-fac9-4abd-8aac-b1a79ae466b9","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:00.230626Z","iopub.execute_input":"2025-05-05T06:11:00.230882Z","iopub.status.idle":"2025-05-05T06:11:01.124566Z","shell.execute_reply.started":"2025-05-05T06:11:00.230861Z","shell.execute_reply":"2025-05-05T06:11:01.123870Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" Initialise the newly added placeholder token with the embeddings of the initializer token","metadata":{"id":"IA3Xj7gBFv-6"}},{"cell_type":"code","source":"token_embeds = text_encoder.get_input_embeddings().weight.data\ntoken_embeds[placeholder_token_id] = token_embeds[initializer_token_id]","metadata":{"id":"0mtxiZMNoQvE","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:01.125277Z","iopub.execute_input":"2025-05-05T06:11:01.125518Z","iopub.status.idle":"2025-05-05T06:11:11.685311Z","shell.execute_reply.started":"2025-05-05T06:11:01.125501Z","shell.execute_reply":"2025-05-05T06:11:11.684390Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In Textual-Inversion we only train the newly added embedding vector, so lets freeze rest of the model parameters here","metadata":{"id":"bQFUhImyFzoS"}},{"cell_type":"code","source":"def freeze_params(params):\n    for param in params:\n        param.requires_grad = False\n\n# Freeze vae and unet\nfreeze_params(vae.parameters())\nfreeze_params(unet.parameters())\n# Freeze all parameters except for the token embeddings in text encoder\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)","metadata":{"id":"vQOYhpPqoS4H","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:11.685984Z","iopub.execute_input":"2025-05-05T06:11:11.686193Z","iopub.status.idle":"2025-05-05T06:11:11.703910Z","shell.execute_reply.started":"2025-05-05T06:11:11.686176Z","shell.execute_reply":"2025-05-05T06:11:11.703213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating our training data","metadata":{"id":"KGfNa4dFF8Om"}},{"cell_type":"markdown","source":"Let's create the Dataset and Dataloader","metadata":{"id":"UG_cy1BrGDIo"}},{"cell_type":"code","source":"train_dataset = TextualInversionDataset(\n      data_root=save_path,\n      tokenizer=tokenizer,\n      size=vae.config.sample_size,\n      placeholder_token=placeholder_token,\n      repeats=100,\n      learnable_property=what_to_teach, #Option selected above between object and style\n      center_crop=False,\n      set=\"train\",\n)","metadata":{"id":"bctHoiRPoWlY","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:11.704618Z","iopub.execute_input":"2025-05-05T06:11:11.704793Z","iopub.status.idle":"2025-05-05T06:11:11.724280Z","shell.execute_reply.started":"2025-05-05T06:11:11.704772Z","shell.execute_reply":"2025-05-05T06:11:11.723612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataloader(train_batch_size=1):\n    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)","metadata":{"id":"MEjOV1VPpB9U","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:11.725075Z","iopub.execute_input":"2025-05-05T06:11:11.725365Z","iopub.status.idle":"2025-05-05T06:11:11.738669Z","shell.execute_reply.started":"2025-05-05T06:11:11.725322Z","shell.execute_reply":"2025-05-05T06:11:11.738026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create noise_scheduler for training","metadata":{"id":"zmlMwmH5GOhJ"}},{"cell_type":"code","source":"noise_scheduler = DDPMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")","metadata":{"id":"0MTkwTNxpRnq","outputId":"e90aa97c-3a3c-4cae-bfe0-9769295b4088","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:11.739305Z","iopub.execute_input":"2025-05-05T06:11:11.739515Z","iopub.status.idle":"2025-05-05T06:11:11.983352Z","shell.execute_reply.started":"2025-05-05T06:11:11.739499Z","shell.execute_reply":"2025-05-05T06:11:11.982659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(noise_scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:11.984156Z","iopub.execute_input":"2025-05-05T06:11:11.984480Z","iopub.status.idle":"2025-05-05T06:11:11.988174Z","shell.execute_reply.started":"2025-05-05T06:11:11.984462Z","shell.execute_reply":"2025-05-05T06:11:11.987575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{"id":"Wbhq4Js2iD2i"}},{"cell_type":"markdown","source":"Define hyperparameters for our training\nIf you are not happy with your results, you can tune the `learning_rate` and the `max_train_steps`","metadata":{"id":"GJ4r0gzvGSg7"}},{"cell_type":"code","source":"#@title Setting up all training args\nhyperparameters = {\n    \"learning_rate\": 1e-4,\n    \"scale_lr\": True,\n    \"max_train_steps\": 4000,\n    \"save_steps\": 200,\n    \"train_batch_size\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"mixed_precision\": \"fp16\",\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output\"\n}\n!mkdir -p sd-concept-output","metadata":{"id":"pnq5vah7pabU","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:11.988893Z","iopub.execute_input":"2025-05-05T06:11:11.989128Z","iopub.status.idle":"2025-05-05T06:11:12.160775Z","shell.execute_reply.started":"2025-05-05T06:11:11.989103Z","shell.execute_reply":"2025-05-05T06:11:12.159924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train!","metadata":{"id":"YNuNDw0wNN5X"}},{"cell_type":"code","source":"#@title Training function\nlogger = get_logger(__name__)\n\ndef save_progress(text_encoder, placeholder_token_id, accelerator, save_path):\n    logger.info(\"Saving embeddings\")\n    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n    learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n    torch.save(learned_embeds_dict, save_path)\n\ndef training_function(text_encoder, vae, unet):\n    train_batch_size = hyperparameters[\"train_batch_size\"]\n    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n    learning_rate = hyperparameters[\"learning_rate\"]\n    max_train_steps = hyperparameters[\"max_train_steps\"]\n    output_dir = hyperparameters[\"output_dir\"]\n    gradient_checkpointing = hyperparameters[\"gradient_checkpointing\"]\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        mixed_precision=hyperparameters[\"mixed_precision\"]\n    )\n\n    if gradient_checkpointing:\n        text_encoder.gradient_checkpointing_enable()\n        unet.enable_gradient_checkpointing()\n\n    train_dataloader = create_dataloader(train_batch_size)\n\n    if hyperparameters[\"scale_lr\"]:\n        learning_rate = (\n            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    optimizer = torch.optim.AdamW(\n        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n        lr=learning_rate,\n    )\n\n    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n        text_encoder, optimizer, train_dataloader\n    )\n\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    # Move vae and unet to device\n    vae.to(accelerator.device, dtype=weight_dtype)\n    unet.to(accelerator.device, dtype=weight_dtype)\n\n    # Keep vae in eval mode as we don't train it\n    vae.eval()\n    # Keep unet in train mode to enable gradient checkpointing\n    unet.train()\n\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n    losses = []\n\n    for epoch in range(num_train_epochs):\n        text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(text_encoder):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample().detach()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states.to(weight_dtype)).sample\n\n                 # Get the target for loss depending on the prediction type\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n                loss = F.mse_loss(noise_pred, target, reduction=\"none\").mean([1, 2, 3]).mean()\n                accelerator.backward(loss)\n\n                # Zero out the gradients for all token embeddings except the newly added\n                # embeddings for the concept, as we only want to optimize the concept embeddings\n                if accelerator.num_processes > 1:\n                    grads = text_encoder.module.get_input_embeddings().weight.grad\n                else:\n                    grads = text_encoder.get_input_embeddings().weight.grad\n                # Get the index for tokens that we want to zero the grads for\n                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n                if global_step % hyperparameters[\"save_steps\"] == 0:\n                    save_path = os.path.join(output_dir, f\"learned_embeds-step-{global_step}.bin\")\n                    save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n\n            logs = {\"loss\": loss.detach().item()}\n            losses.append(loss.item())\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = StableDiffusionPipeline.from_pretrained(\n            pretrained_model_name_or_path,\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            tokenizer=tokenizer,\n            vae=vae,\n            unet=unet,\n        )\n        pipeline.save_pretrained(output_dir)\n        # Also save the newly trained embeddings\n        save_path = os.path.join(output_dir, f\"learned_embeds.bin\")\n        save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n\n    losses_save_path = os.path.join(output_dir, \"training_losses.pt\")\n    logger.info(f\"Saving training losses to {losses_save_path}\")\n    torch.save(losses, losses_save_path)","metadata":{"id":"djBS3343sIiY","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:12.161957Z","iopub.execute_input":"2025-05-05T06:11:12.162238Z","iopub.status.idle":"2025-05-05T06:11:12.180347Z","shell.execute_reply.started":"2025-05-05T06:11:12.162215Z","shell.execute_reply":"2025-05-05T06:11:12.179599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import accelerate\naccelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet),num_processes=1)\n\nfor param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n  if param.grad is not None:\n    del param.grad  # free some memory\n  torch.cuda.empty_cache()","metadata":{"id":"jXi0NdsyBA4S","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:11:12.181066Z","iopub.execute_input":"2025-05-05T06:11:12.181271Z","execution_failed":"2025-05-05T06:11:54.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nlosses_file_path = os.path.join(hyperparameters[\"output_dir\"], \"training_losses.pt\")\nlosses = []\nif os.path.exists(losses_file_path):\n    losses = torch.load(losses_file_path)\n    \nplt.plot(losses)\nplt.xlabel(\"Training Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.grid(True)\nplt.savefig(os.path.join(hyperparameters[\"output_dir\"], \"loss_curve.png\"))  # lưu hình\nplt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run the code with your newly trained model\nIf you have just trained your model with the code above, use the block below to run it\n\nTo save this concept for re-using, download the `learned_embeds.bin` file or save it on the library of concepts.\n\nUse the [Stable Conceptualizer notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb) for inference with persistently saved pre-trained concepts","metadata":{"id":"50JuJUM8EG1h"}},{"cell_type":"code","source":"#@title Set up the pipeline\nfrom diffusers import DPMSolverMultistepScheduler\npipe = StableDiffusionPipeline.from_pretrained(\n    hyperparameters[\"output_dir\"],\n    scheduler=DPMSolverMultistepScheduler.from_pretrained(hyperparameters[\"output_dir\"], subfolder=\"scheduler\"),\n    torch_dtype=torch.float16,\n).to(\"cuda\")","metadata":{"id":"2CMlPbOeEC09","outputId":"afadb546-939e-45cd-a607-7186f5d9147c","trusted":true,"execution":{"execution_failed":"2025-05-05T06:11:54.595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Run the Stable Diffusion pipeline\n#@markdown Don't forget to use the placeholder token in your prompt\n\nprompt = \" photorealistic portrait of sks woman smiling slightly, natural light, sharp focus, high quality, looking at the camera, simple grey background \" #@param {type:\"string\"}\nnegative_prompt = \"deformed, blurry, bad anatomy, disfigured, poorly drawn face, poorly drawn hands, low quality, worst quality, text, signature, watermark, username, drawing, sketch, cartoon, illustration\"\nnum_samples = 2 #@param {type:\"number\"}\nnum_rows = 1 #@param {type:\"number\"}\n\nall_images = []\nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=30, guidance_scale=7.5).images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_rows, num_samples)\ngrid","metadata":{"id":"E3UREGd7EkLh","trusted":true,"execution":{"execution_failed":"2025-05-05T06:11:54.595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"!pip install -qq diffusers transformers ftfy accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:50:23.808263Z","iopub.execute_input":"2025-05-05T07:50:23.808509Z","iopub.status.idle":"2025-05-05T07:51:32.324406Z","shell.execute_reply.started":"2025-05-05T07:50:23.808489Z","shell.execute_reply":"2025-05-05T07:51:32.323739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\nimport PIL\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionPipeline,AutoencoderKL\nfrom transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:51:32.325859Z","iopub.execute_input":"2025-05-05T07:51:32.326179Z","iopub.status.idle":"2025-05-05T07:51:55.866588Z","shell.execute_reply.started":"2025-05-05T07:51:32.326159Z","shell.execute_reply":"2025-05-05T07:51:55.865801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\npretrained_model_name_or_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\nvae_name = \"stabilityai/sd-vae-ft-mse\"\nvae = AutoencoderKL.from_pretrained(vae_name, torch_dtype=torch.float16)\npipe = StableDiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path,\n    vae = vae,\n    torch_dtype=torch.float16\n).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:51:55.867315Z","iopub.execute_input":"2025-05-05T07:51:55.867856Z","iopub.status.idle":"2025-05-05T07:52:19.710139Z","shell.execute_reply.started":"2025-05-05T07:51:55.867836Z","shell.execute_reply":"2025-05-05T07:52:19.709528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprompt = \"a portrait of ((sks)) woman wearing a business suit, model photoshoot, professional photo, white background, Amazing Details, Best Quality, Masterpiece, dramatic lighting highly detailed, analog photo, overglaze, 80mm Sigma f/1.4 or any ZEISS lens\"\nnegative_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime), (nsfw)+, text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry\"\nnum_samples = 2\nnum_rows = 2\nseed = 42  \noutput_dir = \"generated_images\"\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n# Loop through versions from 200 to 2000 with step 200\nfor version in range(200, 4200, 200):  # 200, 400, ..., 2000\n    # Construct the repo_id_embeds path\n    repo_id_embeds = f\"/kaggle/working/sd-concept-output/learned_embeds-step-{version}.bin\"\n    \n    # Clear previous embeddings (reset textual inversion)\n    pipe.unload_textual_inversion() \n    \n    # Load the new textual inversion embeddings\n    pipe.load_textual_inversion(repo_id_embeds)\n    \n    # Create a generator with the specified seed\n    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n    \n    # Generate images\n    all_images = []\n    for _ in range(num_rows):\n        images = pipe(\n            prompt,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_samples,\n            num_inference_steps=100,\n            guidance_scale=10,\n            generator=generator  # Pass the generator with the seed\n        ).images\n        all_images.extend(images)\n    \n    # Create and display the image grid\n    grid = image_grid(all_images, num_samples, num_rows)\n    grid_filename = f\"generated_images/version_{version}.png\"\n    grid.save(grid_filename)\n    # Display the grid for the current version\n    plt.figure(figsize=(10, 10))\n    plt.title(f\"Images for learned_embeds-step-{version}.bin (Seed: {seed})\")\n    plt.imshow(grid)\n    plt.axis('off')\n    plt.show()\n\n# Clean up\ndel pipe\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-05T08:00:40.758Z"}},"outputs":[],"execution_count":null}]}